{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c6c166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D,\\\n",
    "\tMaxPool2D, Conv2DTranspose, Input, Activation,\\\n",
    "\tConcatenate, CenterCrop\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import schedules, Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ff80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Training and Testing Sets\n",
    "\n",
    "train_address_input = r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\Drive_crop\\images\"\n",
    "train_address_vessel = r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\Drive_crop\\vessel\"\n",
    "train_data_ip = []\n",
    "train_data_vessel = []\n",
    "img_names_train = []\n",
    "\n",
    "# test_address_input = r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\RITE_Dataset\\test\\images\"\n",
    "# test_address_vessel = r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\RITE_Dataset\\test\\vessel\"\n",
    "# test_data_ip = []\n",
    "# test_data_vessel = []\n",
    "# img_names_test = []\n",
    "\n",
    "for img in os.listdir(train_address_input):\n",
    "    img_name = os.path.join(train_address_input, img)\n",
    "    img_names_train.append(img_name)\n",
    "    img_names_train.append(img_name)\n",
    "    img = cv2.imread(img_name)\n",
    "    \n",
    "#     img = cv2.resize(img, (360, 360))\n",
    "    train_data_ip.append(img)\n",
    "\n",
    "for img in os.listdir(train_address_vessel):\n",
    "    img_name = os.path.join(train_address_vessel, img)\n",
    "    img = cv2.imread(img_name)\n",
    "#     img = cv2.resize(img, (360, 360))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.flatten()\n",
    "    train_data_vessel.append(img)\n",
    "    \n",
    "# for img in os.listdir(test_address_input):\n",
    "#     img_name = os.path.join(test_address_input, img)\n",
    "#     img_names_test.append(img_name)\n",
    "#     img = cv2.imread(img_name)\n",
    "# #     img = cv2.resize(img, (360, 360))\n",
    "#     test_data_ip.append(img)\n",
    "#     img_names_test.append(img_name)\n",
    "\n",
    "# for img in os.listdir(test_address_vessel):\n",
    "#     img_name = os.path.join(test_address_vessel, img)\n",
    "#     img = cv2.imread(img_name)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img = img.flatten()\n",
    "# #     img = cv2.resize(img, (360, 360))\n",
    "#     test_data_vessel.append(img)\n",
    "\n",
    "    \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e73a1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img)\n",
    "# plt.title('my picture')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bfa13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_ip[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349a6b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3211392   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16384)             2113536   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,418,176\n",
      "Trainable params: 5,418,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "NUM_CATEGORIES = 16384\n",
    "\n",
    "\n",
    "\n",
    "    # defining a model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Convolution and MaxPooling\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "    # Flatten\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "model.add(flatten)\n",
    "\n",
    "    #Hidden layer\n",
    "hidden =  tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "model.add(hidden)\n",
    "dropout = tf.keras.layers.Dropout(0.2)\n",
    "model.add(dropout)\n",
    "\n",
    "    #output layer\n",
    "output = tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "model.add(output)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65097c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "132/132 [==============================] - 63s 472ms/step - loss: 56264658944.0000 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 63s 480ms/step - loss: 5603187490816.0000 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 65s 493ms/step - loss: 68314130808832.0000 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 63s 475ms/step - loss: 353718876766208.0000 - accuracy: 2.3810e-04\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 63s 481ms/step - loss: 1160031475073024.0000 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 70s 529ms/step - loss: 2923059983941632.0000 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 66s 501ms/step - loss: 6396581767020544.0000 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 69s 522ms/step - loss: 11605255036862464.0000 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 64s 483ms/step - loss: 19985661796810752.0000 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 62s 471ms/step - loss: 32384216620597248.0000 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x212b17fa1c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ip=np.array(train_data_ip)\n",
    "train_vessel=np.array(train_data_vessel)\n",
    "model.fit(train_ip, train_vessel, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc3b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip=np.array(train_data_ip)\n",
    "train_vessel=np.array(train_data_vessel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "070fd5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 16384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vessel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06c13eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4200, 16384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vessel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a664173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 16s 122ms/step\n"
     ]
    }
   ],
   "source": [
    "y=model.predict(train_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3123887",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(y[2], return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d25a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16383,     1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbc0f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 16384 into shape (584,565)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m t \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[1;32m----> 3\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m584\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m565\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 16384 into shape (584,565)"
     ]
    }
   ],
   "source": [
    "t = y[0]*255\n",
    "\n",
    "ans = t.reshape(584, 565)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd612190",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740687ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gray_img = t.convert(\"L\")\n",
    "# plt.imshow(gray_img, cmap='gray')\n",
    "\n",
    "img = Image. fromarray(ans, 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.save('my1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv2.imread('my.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52030b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues, occurCount = np.unique(ans, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4438baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"conv2d_9\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_9/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_9/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_9\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39madd(max_pool)\n\u001b[0;32m     32\u001b[0m convolution \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(IMG_WIDTH, IMG_HEIGHT, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m convolution \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(IMG_WIDTH, IMG_HEIGHT, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39madd(convolution)\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\users\\dhyut\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1969\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1966\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1968\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1969\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"conv2d_9\" (type Conv2D).\n\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_9/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_9/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,32], [3,3,32,32].\n\nCall arguments received by layer \"conv2d_9\" (type Conv2D):\n  • inputs=tf.Tensor(shape=(None, 2, 2, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "NUM_CATEGORIES = 16384\n",
    "\n",
    "\n",
    "\n",
    "# defining a model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Convolution and MaxPooling\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(64, (3, 3),strides=(2,2), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(64, (3, 3),strides=(2,2), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(64, (3, 3),strides=(2,2), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "convolution = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "model.add(convolution)\n",
    "max_pool = tf.keras.layers.MaxPooling2D(pool_size = (2,2))\n",
    "model.add(max_pool)\n",
    "\n",
    "    # Flatten\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "model.add(flatten)\n",
    "\n",
    "    #Hidden layer\n",
    "hidden =  tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "model.add(hidden)\n",
    "dropout = tf.keras.layers.Dropout(0.2)\n",
    "model.add(dropout)\n",
    "\n",
    "    #output layer\n",
    "output = tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
    "model.add(output)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip=np.array(train_data_ip)\n",
    "train_vessel=np.array(train_data_vessel)\n",
    "model.fit(train_ip, train_vessel, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2daaf3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root,dirs,files in os.walk(r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\output_vessel\"):\n",
    "    for file in files:\n",
    "        oldname = file\n",
    "        oldname = os.path.join(r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\output_vessel\", oldname)\n",
    "        file_=file.split(\".\")\n",
    "        newname = file_[0] + \"_op.png\"\n",
    "        newname = os.path.join(r\"C:\\Users\\dhyut\\SEM 7\\DL Project\\output_vessel\", newname)\n",
    "        os.rename(oldname, newname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7747bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
